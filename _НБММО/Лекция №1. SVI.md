# Лекция №1. Стохастический вариационный вывод

$p(x|\theta)$

$I(\theta)$ - **[[Информация Фишера|информация Фишера]]** в одномерном случае
$F(\theta)$ - матрица информации Фишера в многомерном случае

Инф-я Фишера связана с **[[Неравенство Рао-Крамера|нер-вом Рао-Крамера]]**: *дисперсию на неизвестный парам-р нельзя сделать меньше, чем $I^{-1}(\theta)$, т.е. чем больше информации, тем меньше можно сделать дисперсию несмещ. оценки.*

Если распр-ие из [[Экспоненциальный класс распределений|экспоненциального класса]], то не нужно матожидать для $F(\theta)$, т.е. всегда можем посчитать.

Натуральный градиент
Проблема:
Обычный градиент (показывает направление наискорейшего роста ф-ции) не инвариантен относительно перепараметризаций (круглая гауссиана ($grad$ смотрит в центр) -> овальная ($grad$ будут осциллировать)).

Решения:
1. Домножение на положит. мат. предобуславливания. В таком случае мы оптимальнее увеличиваем зн-я, но не факт, что наискорейшим образом. (если мат. обратный **[[Гессиан|гессиан]]** -> **[[Ньютоновские методы 2го порядка]]**)

2. Скорректировать опр-ие градиента так, чтобы он был инвариантен к перепараметризациям. Важно то, что оптимизируемый фун-л зав-ит не просто от каких-то параметров, а от параметров, определяющих некоторое распр-ие. Так гораздо лучше, потому что нормальное распр-ие можно параметризовать разными способами, но распр-ие от этого не поменяется. Некоторым подобием метрики для распр-ий явл-ся **[[KL-дивергенция]]**, которая явл-ся метрикой при малых значениях. Так вводится **натуральный градиент** $nat \ grad$. Его использование сильно сокращает кол-во итераций обучения НС, но вычислять очень долго (и даже аппроксимации долго).
	$$
	nat grad \varPhi = F^{-1}(\theta) grad \varPhi
  $$

Рас-им след. модель [[Лекция №10. LDA|латентного размещения Дирихле]]:
$$
p(X,Z,\theta) = p(X,Z|\theta) p(\theta) = \prod_{i=1}^n p(x_i,z_i|\theta)p(\theta)
$$
Пусть распр-ие $p(x_i,z_i|\theta)$ из эксп-го класса и пусть есть условное сопряжение на $Z|\theta$ и на $\theta|Z$ =>  можем применить [[Лекция №7. MFA|MF-аппроксимацию]]. Тогда: $p(Z,\theta|X) \approx q(Z)q(\theta)$.
Получаем выражения на $q(Z)$ и $q(\theta)$, пересчитывая которые, мы будем итеративно сходиться (для $q(\theta)$пересчитываем $\eta_1$ {это сделать легко} и $\nu_1$ {тут надо проходиться по всей выборке}). Но в процессе пересчёта каждого распр-я используется проход по всей обучающей выборке.

А теперь допустим, что $n \gg 1$ - очень много данных (напр, википедия). Т.е. на одной итерации мы 2 раза должны пройти по всему корпусу данных. Будет очень и очень долго. Т.е. классический метод не масштабируется.

С помощью MF мы пытаемся минимизировать KL, но делаем это в классе факторизованных распр-ий, т.к. в классе всех распр-ий полный байесовский вывод невозможен.  В случае MF полный байесовский вывод возможен на одну группу параметров при фикс. остальных, что эквив-но блочнокоординатному полному байесовскому выводу. Но это не единственный способ реш-я задачи:  минимизация KL эквив-на максимизации ELBO, которую можно оптимизировать (например, с помощью градиентных методов).

Вообще, нас интересует не столько из какой темы пришло каждое слово $Z$, сколько вид тематического профиля $\theta$, поэтому хотелось бы настраивать именно $q(\theta)$. Будем оптимизировать ELBO по параметру $\eta$ чтобы не пересчитывать его постоянно. Вычисляя $grad$ ELBO, получаем выражение гессиан умножить на $(...)$. Воспользовавшись определением $nat \ grad$ остаётся только $(...)$, в которой присутствует сумма по всей выборке. Переходим от честного $nat \ grad$ к градиенту по минибатчу $stoch \ nat \ grad$. И никаких сумм считать не надо вообще!

*Алгоритм* **SVI (Stochastic Variational Inference)**:
1) $j \sim U\{1,...,n\}$
2) Считаем $\log q(z_j)$
3) Пересчитываем $\eta_{t+1} = \eta_t + \varkappa_t(...)$
	$\varkappa_t$ удовлетворяют условиям [[Теорема Робинсона-Монро|т-мы Робинсона-Монро]] 
    $\nu_{t+1}$ пересчитывается очень просто

*Общая идея:*
Масштабируемый метод сводим к задаче оптимизации, которую сводим к градиентной оптимизации.