# Лекция №3. Variational Dropout
**Dropout** - процедура зануления весов с некоторой вероятностью $p$ - регуляризация, которая препятствует ко-адаптации нейронов. Фактически каждый раз разыгрывается сл.в. из $Be(p)$, которая домножается на очередной вес. Предложена в 2012 году. Тогда не понимали как это работает.

$$
z_i = \sum_j{w_{ij}\varepsilon_{ij}y_j}
$$

*А что если разыгрывать не Бернулевскую сл.в., а Гауссовскую?*
Так появилось улучшение - **Гауссовский dropout:**
$$
\varepsilon_{ij} \sim \mathcal{N}(\varepsilon_{ij}|1, \alpha)
$$
Фактически эквивалентен классическому **бинарному dropout** т.к. сумма нормальных распр-ий - нормальное, сумма большого числа н.о.р.с.в. ~ нормальное (из ЦПТ).

---

*Задача классификации.*
Посчитаем стохастический (по $\varepsilon$ и minibatch) градиент.

*А что мы оптимизируем?*
Узнаем, посчитав честный градиент ($\mathbb{E}$ по всей случайности). А оптимизируем мы ансамбль по всем весам $w$ log-правдоподобий. Оптимизация происходит ==только== по весам: по регуляризации $\alpha$ не можем, т.к. любая регуляризация препятствует более точной настройке под данные.

*Раньше (со времён Тихонова):*
Под регуляризацией понимали аддитивную добавку ($L_1, L_2$ и т.д.)

*Сейчас:*
Поняли, что нужно не добавлять, а "впрыскивать" шум.
*(Так делают в BatchNorm, в DropOut и даже в SGD!)*

*На что похож честный градиент?*
Полученный честный градиент напоминает первое слагаемое **[[Лекция №6. EM#^4a13f4|ELBO]]**, если его разложить в два слагаемых. Но для байесовского подхода необходимо как минимум задать априорное рапр-ие на параметры $w$. А даже если мы и введём такое рапр-ие, то совсем не гарантируется, что оптимизация всего **ELBO** будет экв-на оптимизации только его первого слагаемого.
==Но== если удастся подобрать априорное распр-ие так, чтобы второе слагаемое ELBO (- KL-дивергенция) занулилась, то задачи будут экв-ны.

*Какое априорное распр-ие взять?*
1. *Не очень честное реш-ие.*
Изначально взяли несобственное **log-uniform** распр-ие:
$$
p(w) \propto \frac1{|w|}
$$
С ним связан **парадокс Бетфорда** или парадокс первой цифры.

2. *Честное реш-ие.*
$$
p(w|\Lambda) = \prod_{ijk} \mathcal{N}(w_{ijk}|0, \lambda^2_{ijk})
$$
Будем подбирать $\Lambda$ с помощью принципа **[[Лекция №3. Наибольшая боснованность|наибольшей обоснованности]]**. Её посчитать в явном виде не можем, но можем оптиимизировать **ELBO**. Теперь оптимизация может происходить и по параметрам регуляризации, т.к. уже не поощряем их отстутствие.

*Улучшение:*
Можем сделать обобщение - дисперсия регуляризации $\alpha_{ijk}$ своя для каждого веса $\mu_{ijk}$. В процессе сталкиваемся с некоторыми численными трудностями, а именно: большое плечо при выбранной изначально параметризации $(\mu, \alpha)$. Реш-ие - репараметризация $(\mu, \sigma)$. ^e47676

*Итог.*
Метод **Sparse Variational Dropout** (arxiv: ) - Ветров, Молчанов и Ко 2019, который очень сильно прореживает сеть (в 100 и 1000 раз) и идейно похож на **[[Лекция №5. RVM для классификации|RVM]]**, но работает ==только== с перепараметризованными моделями. При этом оптимизируется только память, но не скорость (пока нет эффективных алгоритмов работы с разреженными матрицами). Применение к пучкам весов - нейронам настолько выдающегося результата не приносит (в 2-3 раза).