# Семинар №3. Variational Dropout
Семинар к [[Лекция №3. Variational Dropout|Лекции №3]].

Сравнение [[Лекция №3. Variational Dropout#^e47676|двух равноценных параметризацией]]:
* $(\alpha, \mu)$ - тут возникает слагаемое, которое представляет собой произведение очень большого числа на маленькое число, меняющее знак - реализация сл.в. => очень большая $\mathbb{D}$;
* $(\mu, \sigma)$ - тут этой проблемы нет.

Рассмотрен вид соответствующих им KL-дивергенций.

Градиент можем прокидать в силу использования **parametrization trick** - представление сл.в. весов в виде отдельно нестохастических весов и стохастической сл.в., которая от этих весов не зав-т.

Сравнение **parametrization trick** и **local parametrization trick** - сэмплируем не одну матрицу весов для всех объектов обучающей выборки, а для каждого объекта свою. Второй подход лучше, т.к. $\mathbb{D}$ меньше (нет слагаемого с $cov$, при котором квадратичная зав-ть от размера батча), но так медленнее.

