# Лекция №2. Дважды стохастический вариационный вывод
Мы знаем, что делать, если очень много данных $N$ и немного параметров $d$ *(недопараметризованная модель)* - апостериорная вер-ть байесовского подхода коллабсируется в $\delta(\theta - \theta_{MLE})$.

*А что делать, если параметров тоже очень много (перепараметризованная модель)?*
Практика показывает, что чем больше параметров, тем лучше результат (при правильном обучении), что, вообще говоря, противоречит классическому ML. У какой-нибудь GPT-4 точно)))0) нет сопряжения, а хотим сделать байесовский вывод. *(Если бы было какое-нибудь - применили [[Лекция №1. SVI|стохастический вариационный вывод]])*

*Что делать?*
Мы хотим получить апостериорную вероятность. Точную мы, конечно, не получим, но приблизить вероятностью из какого-то параметрического семейства можем:
$$
p(\theta|X) \approx q(\theta|\varphi)
$$

*Как мы численно оцениваем насколько распр-я похожи?*
Правильно, с помощью KL-дивергенции!

*Почему с помощью именно этой дивергенции, хотя есть много других?*
Потому что для того посчитать численно любую дивергенцию необходимо знать апостериорное распр-ие, которое мы не знаем. А KL связана с ELBO: минимизация KL экв-на максимизации ELBO. Т.о. задача сводится к максимизации ELBO. Хотя там тоже инт-л не берётся, но зато мы можем посчитать подинт-ое выражение. Да и чтобы оптимизировать интегральное выр-ие уметь считать инт-л не нужно - достаточно уметь считать стохастический градиент и оптимизироваться по нему.

*Проблема:*
Мы знаем как брать $stoch \ grad$ в разных случаях:
* если есть сумма ф-ций => используем minibatch
* если есть мат. ожидание => оценка Монте-Карло

*А что делать, когда мы дифф-ем мат. ожидание по параметру при этом плотность мат. ожид. зав-т от этого же пар-ра?*
Использовать M-K уже не получится: оценка градиента будет смещённой.
Поэтому, пробрасывая дифф-ие внутрь мат. ожид-я, разбиваем его на два мат. ожид-я, используя **log derivate trick** (этот приём наз-ся **REINFORCE**). В силу факторизации правдоподобия, применяется переход к стохастике сначала для сэмплирования minibatch объектов данных, а затем для оценки М-К. Этот приём наз-ся **дважды стохастическим вариац. выводом**.

Отличие в применениях стох. вар. вывода и дважды стох. вар. вывода:
В случае, когда можно явно посчитать апостериорное распределение (есть сопряжение) (в том числе и в блочнокоординатном случае [[Лекция №7. MFA|MF]]) стохастика используется только для взятия minibatch объектов из всей выборки, т.к. запись ELBO задаётся в явном виде.
В случае же, когда апостериорное распр-ие не представляется в явном виде, ELBO также не имеет явного представление требуется ещё одна стохастика по М-К для избавления от интеграла.
В этом вся разница применений, вот так просто, да!

Проблема REINFORCа в очень большой дисперсии. Она получается из-за домножения случайной величины со средним $0$ на ф-цию, содержащую в качестве множителя объём данных $N$.

Вычитая из этой ф-ции её среднее, дисперсия сильно уменьшается. (К оценке градиента можно добавлять всё со средним мат. ожиданием равным 0, несмещённость сохраняется.) Этот метода наз-ся **REINFORCE+**. Но этот метод всё равно проигрывает **reparametrization trick**.

*Практическое применение:* [[Лекция №5. RVM для классификации|RVM]]
