# Лекция №2. Дважды стохастический вариационный вывод
Мы знаем, что делать, если очень много данных $N$ и немного параметров $d$ (недопараметризованная модель) - апостериорная вер-ть байесовского подхода коллабсируется в $\delta(\theta - \theta_{MLE})$.

*А что делать, если параметров тоже очень много (перепараметризованная модель)?*
Практика показывает, что чем больше параметров, тем лучше результат (при правильном обучении), что, вообще говоря, противоречит классическому ML. У какой-нибудь GPT-4 точно)) нет сопряжения, а хотим сделать байесовский вывод.

*Что делать?*
Мы хотим получить апостериорную вероятность. Точную мы, конечно, не получим, но приблизить вероятностью из какого-то параметрического семейства можем:
$$
p(\theta|X) \approx q(\theta|\varphi)
$$

Как мы численно оцениваем насколько распр-я похожи?
С помощью KL-дивергенции.
Почему с помощью именно этой дивергенции, хотя есть много других?
Потому что для того посчитать численно дивергенцию необходимо знать апостериорное распр-ие, которое мы не знаем и не можем вычислить. А KL связана с ELBO: минимизация KL экв-на максимизации ELBO. Т.о. задача сводится к максимизации ELBO. Хотя там тоже инт-л не берётся, но зато мы можем посчитать подинт-ое выражение. Да и чтобы оптимизировать интегральное выр-ие уметь считать инт-л не нужно - достаточно уметь считать стохастический градиент и оптимизироваться по нему.

*Проблема:*
Мы знаем как брать $stoch \ grad$ в разных случаях:
* если есть сумма ф-ций => используем minibatch
* если есть мат. ожидание => оценка Монте-Карло

А что делать, когда мы дифф-ем мат. ожидание по параметру. При этом плотность мат. ожид. зав-т от этого же пар-ра? Использовать M-K уже не получится: оценка градиента будет смещённой.

Поэтому мат. ожид-е разбиваем на два мат. ожид-я, используя **log derivate trick**, пробрасывая дифф-ие внутрь. И тут в силу наличия правдоподобия, которое факторизуется, применяется переход к стохастике сначала для сэмплирования minibatch объектов данных, а потом для оценки М-К. Этот приём наз-ся **дважды стохастическим вариац. выводом**. А способ считать стохастический градиент в аналогичных нашей ситуациях - **REINFORCE**

Проблема REINFORCа в очень большой дисперсии. Так получается из-за домножения случайной величины со средним $0$ на ф-цию, содержащую в качестве множителя $N$ - объём данных.

Вычитая из этой ф-ции её среднее дисперсия очень сильно уменьшается. Этот метода наз-ся **REINFORCE+**

*Практическое применение:* [[Лекция №5. RVM для классификации|RVM]]
