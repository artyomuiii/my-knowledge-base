# MARS
**Multivariate Adaptive Regression Splines**
*(Jerome H. Friedman 1991)*

Аннотация на русском:
1. В машинном обучении много разных методов, каждый из которых обладает своей областью применения, преимуществами и недостатками. Развитие и переосмысление старых, а также предложение новых методов - вот какие задачи ставятся перед учёными в области теоретического машинного обучения.
2. Авторы данной фунадменатльной статьи предлагают новый метод MARS (Multivariate Adaptive Regression Splines), т.к. считают, что ниша похожих на MARS алгоритмов ещё не занята и хотят это исправить, чувствуя в этом типе методов большую перспективу.
3. Новый алгоритм представляет из себя многомерную непараметрическую регрессию и обобщает ранее существовавшие наработки в этом направлении. Идея заключается в итеративном построении базисных функций, представляемых в виде произведения однотипных элементарных функций. В качестве последних авторы предлагают использовать функцию положительной срезки или, по-другому, ReLU. Такие базисные функции задают многократный излом многомерной линейной функции, позволяя грубо аппроксимировать любую зависимость в данных. По ходу статьи авторы предлагают эффективные численные приёмы и улучшения для своего метода, а также расширяют его на разные случаи, которые могут потребоваться пользователю. Например, специальные элементарные функции для сохранения требуемой гладкости итоговой функции.
4. Для определённого вида задач демонстрируются прекрасные результаты. Показывается, что данный метод имеет место быть и занимает определённую нишу в иерархии классических методов машинного обучения.
5. Авторы активно дают советы по дельнейшему улучшению и расширению метода.

Аннотация на английском:
1. There are many different methods in machine learning, each of which has its own field of application, advantages and disadvantages. The development and reinterpretation of old methods, as well as the proposal of new methods - these are the tasks set for scientists in the field of theoretical machine learning.
2. The authors of this fundament article propose a new MARS method (Multivariate Adaptive Regression Splines), because they believe that the niche of MARS-like algorithms is not yet occupied and want to fix it, feeling a great prospect in this type of methods.
3. The new algorithm is a multidimensional nonparametric regression and generalizes previously existing developments in this direction. The idea is to iteratively construct basic functions represented as a product of the same type of elementary functions. As the latter, the authors suggest using the positive slicing function or, in another way, ReLU. Such basic functions define a multiple fracture of a multidimensional linear function, allowing you to roughly approximate any dependence in the data. In the course of the article, the authors propose effective numerical techniques and improvements for their method, as well as expand it to different cases that the user may need. For example, special elementary functions to preserve the required smoothness of the final function.
4. Excellent results are demonstrated for a certain type of task. It is shown that this method has a place to be and occupies a certain niche in the hierarchy of classical machine learning methods.
5. The authors actively give advice on how to improve and expand the method.

Заключение на русском:
В заключение хочу отметить, что данная статья 

Заключение на английском:

## 2. Existing methodology

#### 2.1. Global parametric modeling
![[Pasted image 20221027204321.png]]
***
#### 2.2. Nonparametric modeling
![[Pasted image 20221027205111.png]]
#### Local parametric approximations (smoothers)
![[Pasted image 20221027205933.png]]
***
#### 2.3. Low dimensional expansions
![[Pasted image 20221027230535.png]]
![[Pasted image 20221027230624.png]]
***
#### 2.4.1 Projecticn pursuit regression
![[Pasted image 20221027235818.png]]
![[Pasted image 20221028000003.png]]
***
#### 2.4.2 Recursive partitioning regression (RPR)
![[Pasted image 20221029151454.png]]
![[Pasted image 20221029181421.png]]
![[Pasted image 20221030004559.png]]
***
***


## 3. Adaptive regression splines

#### 3.1. Recursive partitioning regression revisited
**MARS** - это многомерная непараметрическая (чем больше данных, тем лучше работает) регрессия (обобщение **Recursive partitioning regression (RPR)**).
***
![[Pasted image 20221029201615.png]]
$LOF(g)$ - процедура, вычисляющая несоответствие ф-ции $g(x)$ данным.
![[Pasted image 20221029202129.png]]

Находим одну координату $\mathbf{x}$ и одно значение этой координаты на всех эл-ах обуч. выборки для одной из имеющихся базисных функций, такие что $LOF(g)$, оптимизированная по множителям $a_1, ..., a_M$, (задача лин. регрессии) оказывается с самым маленьким значением. В качестве новых б. ф. выступают старая ф-ция, домноженная на $H$ с противоположными по знаку аргументами.

![[Pasted image 20221030005227.png]]
![[Pasted image 20221030005637.png]]
Сначала генерируется большое кол-во б. ф., затем происходит процедура удаления тех ф-ций, которые не вносят ощутимый вклад в качество модели. Просто удалять ф-ции не получится, т.к. тогда образуются "дыры".
***
#### 3.2. Continuity
Проблема **RPR** - отсутствие непрерывности на границах субрегионов. Можно модифицировать алг. 1, но для этого также необходимо обобщить кусочно-постоянную ф-цию  $H$ непрерывным аналогом (кус-непр ф-ция - частный случай сплайновых б.ф.).

**Сплайн** — [функция](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0) "Функция (математика)"), область определения которой разбита на конечное число отрезков, на каждом из которых она совпадает с некоторым алгебраическим [полиномом](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC "Полином"). Максимальная из степеней использованных полиномов называется **степенью сплайна**. Разность между степенью сплайна и получившейся [гладкостью](https://ru.wikipedia.org/wiki/%D0%93%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%D1%8F_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F "Гладкая функция") называется **дефектом сплайна**.

![[Pasted image 20221030014622.png]]
![[Pasted image 20221104011735.png]]
![[Pasted image 20221030112625.png]]
Но так появляется много других проблем, которые надо как-то решить.
***
#### 3.3. A further generalization
*Требуется: найти небольшой (чтобы дисперсия была маленькой) набор б.ф., который хорошо описывает исходную зав-ть (чтобы смещение было маленьким).*

Ещё одна проблема **RPR** - неспособность хорошо приближать достаточно простые ф-ции (в частности линейные и аддитивные), зависящие от небольшого числа переменных.

*Геометрическая интерпретация проблемы:*
Необходимо очень много гиперпрямоугольников, ориентированных по осям, чтобы выучить функциональную зависимость ф-ций, которые в каждом прямоугольнике ориентированы под углом к осям.

*Почему возниакет проблема?*
Использование кус.-пост. ф-ций.

*Ещё одна проблема:*
Б.ф. обычно зависят от большого числа переменных  (т.к. родительская ф-ция, зависящая от $n$ переменных, в общем случае заменяется на ф-цию, зависящую от $n + 1$ переменной), поэтому ф-ции, зависящие от малого числа переменных, плохо аппроксимируются.

*Идея:*
Не удалять родительскую б.ф. (т.е. каждый раз кол-во б.ф. увеличивается на $2$). Т.о. б.ф. $B_1(x) = 1$ также включается, что позволяет создавать аддитивные модели и, в частности, линейные.

Также решается проблема многократного разделения по одной и той же переменной. Т.к. иначе результирующий базис не сведётся к набору нужных сплайновых б.ф. (степень при переменной не будет соответствовать степени сплайна). Такое разделение происходило раньше из-за невозможности адекватной аппроксимации аддитивных ф-ций, но теперь эта проблема решена.

*Итоговые модификации алгоритма 1:*
1. Замена ступенчатой ф-ции на усеченную степенную сплайновую
ф-цию.
2. Не удалять родительскую б.ф. $B_{m^*}(\textbf{x})$ после ее разделения.
3. В каждом произведении, соотв. б.ф., множители должны зависеть от разных переменных.

Важно также выбрать степень непрерывности итоговой аппрокимации, которая задаётся параметром $q$.
***
#### 3.4. MARS algorithm
Модифицированный алгоритм 1.
![[Pasted image 20221030155511.png]]

Обратный шаг - прореживание набора б.ф. Жадный алгоритм.
На первом шаге удаляем из исходного мн-ва по одной б.ф. Для получившихся мн-в считаем $LOF$.
* Если он улучшился локально => перезаписываем лучшее локальное значение $LOF$ $b$ и лучшее локальное мн-во  $K^*$.
* Если он улучшился глобально => перезаписываем лучшее глобальное зн-е $LOF$ $lof^*$ и лучшее глобальное мн-во $J^*$.

На следующем шаге в качестве мн-ва, из которго будем удалять б.ф., берём лучшее локальное мн-во $K^*$ с предыдущего шага и повторяем процедуру, пока не останется ==одна== константная б.ф. $B_1(x) = 1$, которая должна оставаться в любом мн-ве. В итоге получается вложенная посл-ть моделей с уменьшающимся кол-вом б.ф.

![[Pasted image 20221030161740.png]]
***
#### 3.5. ANOVA decomposition
Приближение целевой ф-ции после проделанных шагов:

![[Pasted image 20221030184603.png]]

Перепишем аппроксимирующую ф-цию в более информативном виде. Для этого сгруппируем б.ф., зависящие от одних и тех же переменных:

 ![[Pasted image 20221030184626.png]]
![[Pasted image 20221030185158.png]]
![[Pasted image 20221030185608.png]]
![[Pasted image 20221030185938.png]]
*Кажется, что в 24 ошибка: сумма нужна не по $m$, а по $i$.*
Данное разложение наз-ся разложением **ANOVA (Analysis of variance)** - анализ дисперсии, часто используется в статистике.
Это разложение более интерпретируемо. По нему сразу видно, какие переменные и в каком функциональном виде входят в модель. При этом есть возможность построения графиков по переменным или их проекциям.
***
#### 3.6. Model selection
Осталось ещё определить критерий несоответствия $LOF$ и максимальное кол-во б.ф. $M_{max}$. 

$LOF:$
Сначала определим ф-цию расстояния:
![[Pasted image 20221031005621.png]]
Использование квадратичной ф-ции объясняется некоторыми вычислительными упрощениями.

**GCV (Generalized Cross-validation Criterion):**
*(Craven and Wahba (1979))*
![[Pasted image 20221031010722.png]]
Штраф в знаменатале нужен для учёта увеличивающейся дисперсии из-за увеличения кол-ва б.ф.

![[Pasted image 20221031010927.png]]

Где смысл $C(M)$ - число л.н. б.ф. *(не знаю почему так, в статье выше ответ)*

*(Friedman and Silverman (1989))*

![[Pasted image 20221102212923.png]]

Где: $d$ - параметр сглаживания (чем больше, тем меньше узлов создаётся). Обычно выбирается из диапазона $2 \le d \le 4$ (стандартный вариант $d = 3$). **GCV** зависит от $d$, а  **MSE** не зависит.

*TODO: Добавить в код простое вычисление $C(M)$ как в книге Statistical learning.*

$M_{max}:$
Обычно $M_{max} = 2M^*$ где $M^*$ - оптимальное кол-во б.ф. (в смысле $GCV$)

#### 3.7. Degree-of-continuity
Иметь первую производную полезно для внешнего вида ф-ции, для увеличения доступных методов оптимизации таких ф-ций и т.д. Для этого необходимо иметь как минимум 2ю степень у сплайновых усечённых б.ф.

Ещё одна проблема - граничные эффекты, вызванные (если я правильно понял) резким переходом в точке излома.

Авторы предлагают вместо:
![[Pasted image 20221105015210.png]]
использовать:
![[Pasted image 20221105015235.png]]

Теперь это уже не линейные, а кубические усечённые сплайны с непрерывными 1ми производными, повторяющие вдали от излома hinge-сплайны.

![[Pasted image 20221105015437.png]]
Кусочно-линейные сплайны задавались одной точкой излома $t$. Данные кубические сплайны задаются той же центральной точкой излома $t$ и ещё 2мя точками $t_-, t_+$ - средние точки между соседними центральными. Так сделано для уменьшения разрывов 2ой производной, которая и так разрывна в центральных точках:


#### 3.8. Knot optimization
Для **ANOVA** разложения верна следующая форма записи:
![[Pasted image 20221105152752.png]]

Тут второе слагаемое в (36) - это вклад набора переменных $V(m)$ и $V(m) + \{v\}$, (37) - вклады всех остальных наборов переменных. Оптимизация в алгоритме происходит по набору параметрам $\{c_j\}$ и по набору параметров $\{a_i\}$.

![[Pasted image 20221105154909.png]]
$R_{mv}$ - остаток, который должен быть приближен $\phi_{mv}$.
Т.е. исходную задачу аппроксимации моделью исходной ф-ции мы свели к задаче аппроксимации остатка, который должен быть приближен в среднем по всем б.ф. и по всем координатам (матожидаем по этим параметрам).
Также, теперь в задаче явно не фигурирует многомерный вектор $x$, а только его отдельные координаты. *(чего?)*

Как видно, в (40) зависимость только от одной переменной *(это если не матожидать)*. Если зафиксировать значения $\{a_i\}$, то (40) имеет общее решение (*почему это общее решение? как его получили?*):

![[Pasted image 20221105155032.png]]

*(Всё далее не претендует на правду)*
Это может быть оценено с помощью взвешенного сглаживания. Если придать сглаживанию форму кусочно-линейного сплайнового приближения, то мы косвенно приходим к **MARS**. Тут есть связь с методом сглаживания **TURBO**. Многие шаги очень похожи (например, выбор $t$).
Если разрешать в качестве порога разбиения $t$ использовать любую точку из обучающих данных, то, в рамках задачи сглаживания, допускается сглаживание по одному наблюдению, что недопустимо, т.к. шум будет сильно влиять. Поэтому хочется найти минимальный интервал (число $L(\alpha)$ данных между каждым узлом), устойчивый к шумам.

![[Pasted image 20221106014639.png]]
$Pr(L^*)$ - вероятность $\ge L^*$ успехов в $nN_m$ испытаниях Бернулли с $p = 0.5$,
$\alpha$ - маленькое число (0.05 или 0.01),
$N_m$ - кол-во наблюдений, для которых $B_m > 0$
*(не понял, в чём смысл $nN_m$)*
$n$ - размерность объекта

*Идея такая:*
Пусть есть некоторый прямоугольник, в котором допускается присутствие $nN_m$ точек. Т.к. наши данные - это истинная ф-ция + некоторый шум со средним в нуле (пусть будет так). Т.о. этот шум в нетривиальном случае обязан иметь чередующиеся знаки. Будем считать, что значение шума есть $+1, -1$. Тогда, мы адекватно приближаем истинную зависимость, если шум в каждой наблюдаемой точке не сделался очень большим с точностью до знака.

![[Pasted image 20221106013309.png]]
![[Pasted image 20221106013323.png]]
$Le(\alpha)$ - аналог  $L(\alpha)$ только уже для внешних порогов, а не внутренних.

#### 3.9. Computational considerations.
Решать задачу **МНК** можно разными способами:
* используя $QR$-разложение. Такой способ хорош численно;
* используя разложение Холецкого $L^TL$ (для симметр. полож. опред. матрицы), который численно менее устойчивый.

*В (46) ошибка - перепутаны размерности.*
![[Pasted image 20221106162530.png]]
![[Pasted image 20221106154238.png]]
![[Pasted image 20221106154410.png]]
Матрица $V$ - как раз симметричная и положительно определённая матрица.

*Замечание:*
При формировании $V$ и $c$ производится нормализация (вычитание среднего).
$$
(B - \overline{B}) a = y - \overline{y}
$$
$$
B^T(B - \overline{B}) a = B^T(y - \overline{y})
$$
$$
Va = c
$$

*Проблема:*
Необходимо много раз считать значения новых двух б.ф. на всех объектах обучающей выборки, а далее решать задачу оптимизации для поиска соответствующего порога $t$ (проход по порогам самый долгий, т.к. необходимо пройти по всем объектам обучающей выборки $N$). Поэтому хотим как угодно, но упростить задачу.

Итоговая сложность **MARS**a "в-тупую":
![[Pasted image 20221106171403.png]]
*(Полностью разобраться в асимптотиках)*

*Идея:*
Для уменьшения вычислений вместо ф-ции $g$ используется ф-ция $g^{'}$, оптимизация которой приводит к тем же $lof^*$ и $t^*$, но значения $\{a\}_i$ будут другими. Преимущество такой ф-ции заключается в том, что при изменении $t$ изменяется только последнее слагаемое.
![[Pasted image 20221106171643.png]]
Если отсортировать возможные пороги, то не трудно получить пересчётные формулы. По ним в целой матрице $V$, которая явл-ся симметричной и положительно определённой, достаточно пересчитать только последнюю строку (она же - последний столбец), а для вектора $c$ - только последний элемент $c_{M+1}$. Оптимальный вектор коэффициентов $a$ находится из системы *(48)*, в которой матрица $V$ раскладывается по методу Холецкого в  $L^TL$, где $L$ - нижнетреугольная матрица. Т.о. оптимизация по $a$ сводится к разложению матрицы $V$ и реш-ю 2х СЛАУ с треугольными матрицами (МНК).
![[Pasted image 20221106172439.png]]
![[Pasted image 20221225222858.png]]

*Выигрыш:*
Теперь вместо того, чтобы проходиться по всем порогам $t$ (эл-ам обуч. выборки), пересчитывая для них новые б.ф. $B_M(x)$ и $B_{M+1}(x)$ ($2N$ операций для обновления матрицы $B$), а потом перемножая $B^T$ и $B$ для получения матрицы $V$ и аналогично для получения вектора $c$, мы  сразу без перемножений пересчитываем последний столбец матрицы $V$, эл-т вектора $c$ и последний столбец матрицы $B$.

Итоговая сложность оптимизированного **MARS**a:
![[Pasted image 20230425003701.png]]

## 4. Эксперименты
##### 4.1 *Может ли сложиться такая ситуация?*
$B_1(x)=1$
$B_2(x)=[+(x_i-t)]_+$
$B_3(x)=[-(x_i-t)]_+$
$B_4(x) = [+(x_i-u)]_+$
$B_5(x)=[-(x_i-u)]_+$

Т.е. создались две б.ф. с одной координатой $x_i$ и порогом $t$, а сразу же после создались ещё две б.ф. с той же координатой, но с другим порогом $u$.

*Идеи:*
скорее всего может быть, т.к. иначе следствие из ANOVA нетривиального смысла.

