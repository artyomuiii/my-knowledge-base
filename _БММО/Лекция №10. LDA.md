## Лекция №10. Латентное размещение Дирихле.
Из названия понятно, что будем работать с [[Распределение Дирихле|распределением Дирихле]].
**Латентное размещение Дирихле (LDA)** применяется для рубрификации текстов.

*Неформальная постановка задачи:*
Дан текст, состоящий из предложений. Каждое предложение состоит из каких-то слов. Из всех слов формируется словарь. Также имеем набор разных тем (*"про космос"*, *"про животных"*, *"про рыцарей"*  и т.д.).
Хотим для каждого текста получить набор тем, которому он соответствует. (*Этот текст про животных в космосе.*)  


*Обозначения:*
$d$ -  номер док-та
$n$ - номер позиции слова в док-те
$t$ - номер темы

$W$ - размер словаря (или сам словарь)
$w_{d_n}$ - слово в док-те $d$ на позиции $n$
$z_{d_n}$ - тема, из которого пришло слово в док-те $d$ на позиции $n$
$\theta_d$ - распр-е тем в док-те $d$
$\phi_t$ - распр-е слов в теме $t$
$\Phi$ - распр-е слов во всех темах


*Вероятностная модель:*



*Дано:* $W$
*Найти:* $\Phi$

Имеем латентные переменные => применяем EM-алгоритм или его модификации: для выписанной модели нет сопряжённости, но есть условная сопряжённость => применяем MF для E-шаге. На M-шаге выполняется условная оптимизация, т.к. строки $\Phi$ - вер-ти.

E-step и M-step делаем сбалансированно по сложности вычислений: если один из шагов сильно быстрее => по нему за раз можно делать несколько итераций.
