**Случайный процесс** — совокупность бесконечного (счетного или континуального) числа случайных величин

Обозначение: $f(x), x \in \mathbb R^d$, где $x$ — индексирующий параметр

- $f(x_0)$ — одномерная проекция процесса $f(x)$

- $\big( f(x_1), \ldots, f(x_n) \big)$ — многомерная проекция процесса $f(x)$

**Функция среднего значения** $m(x) = \mathbb E f(x)$

**Ковариационная функция** $K(x, x') = \operatorname{Cov} \big( f(x), f(x') \big)$
- Ковариационная функция положительно определена:
  $$
  \forall g \in \mathcal L_2 \Rightarrow \iint K(x, x') g(x) g(x') \ dx dx' > 0
  $$

Случайный процесс называется стационарным
- в широком смысле, если $m(x) = const, \ K(x, x') = K(x - x')$ (при этом можно положить $m(x) = 0$)
- в узком смысле, если $p \big( f(x_1), \ldots, f(x_n) \big) = p \big( f(x_1 + \Delta x), \ldots, f(x_n + \Delta x) \big)$

Для стационарного случайного процесса:
- $K(\Delta x) = K(-\Delta x)$
- $K(0) = \mathbb D f$
- $K(0) \ge \big| K(\Delta x) \big|$ (ковариация всегда меньше дисперсии)
- Если $\lim_{\Delta x \to 0} K(\Delta x) = K(0)$, то $\lim_{\varepsilon \to 0} f(x + \varepsilon) = f(x)$, то есть $K(\Delta x)$ непрерывна в 0, то любая реализация случайного процесса является непрерывной функцией, т.к.
  $$
  \lim_{\varepsilon \to 0} \operatorname{Cov} \big( f(x), f(x + \varepsilon) \big) =  \lim_{\varepsilon \to 0} K(\varepsilon) = K(0)
  $$
  $$
  \lim_{\varepsilon \to 0} \operatorname{corr} \big( f(x), f(x + \varepsilon) \big) = 1
  $$

## Гауссовский случайный процесс
Случайный процесс $f(x)$ называется **гауссовским случайным процессом**, если любая его конечномерная проекция имеет нормальное распределение:
$$
\forall n \Rightarrow p \big( f(x_1), \ldots, f(x_n) \big) ) \sim \mathcal N \big( f(x_1), \ldots, f(x_n) \mid \mu, \Sigma  \big)
$$
- Гауссовский случайный процесс однозначно определяется своими $m(x)$ и $K(x, x')$

$$
p \big( f(x_1), \ldots, f(x_n) \big) ) = \mathcal N \left( f(x_1), \ldots, f(x_n) \mid
\begin{pmatrix} m(x_1) \\ \vdots \\ m(x_n) \end{pmatrix},
\begin{pmatrix}
K(x_1, x_1) & \cdots & K(x_1, x_n) \\
\vdots & \ddots & \vdots \\
K(x_n, x_1) & \cdots & K(x_n, x_n)
\end{pmatrix} 
\right)
$$

### Частный случай — белый шум
$$
m(x) = 0, \quad
K(\Delta x) = \begin{cases} 1, \ \Delta x = 0 \\ 0, \ \text{otherwise} \end{cases}
$$
Тогда
$$
p \big( f(x_0) \big) = \mathcal N \big( f(x_0) \mid 0, 1 \big)
$$
и одномерные проекции никак между собой не связаны

### Формулы Андерсена
**Задача:** пронаблюдав реализации $F = \big( f(x_1), \ldots, f(x_n) \big)$ гауссовского случайного процесса $f(x)$ в точках $x_1, \ldots, x_n$ построить вероятностный прогноз $\tilde F = \big( f(y_1), \ldots, f(y_m) \big)$ в точках $y_1, \ldots, y_m$

$$
p \big( \tilde F \mid F \big) = \frac{p \big( \tilde F, F \big)}{p(F)} = \mathcal N \big( \tilde F \mid \mu, \Sigma \big)
$$

**Формулы Андерсена:**
$$
\mu = K^T C^{-1} F, \quad \Sigma = S - K^T C^{-1} K
$$
где
$$
\begin{pmatrix}
K(x_1, x_1) & \cdots & K(x_1, x_n) & K(x_1, y_1) & \cdots & K(x_1, y_m) \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
K(x_n, x_1) & \cdots & K(x_n, x_n) & K(x_n, y_1) & \cdots & K(x_n, y_m) \\
K(y_1, x_1) & \cdots & K(y_1, x_n) & K(y_1, y_1) & \cdots & K(y_1, y_m) \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
K(y_m, x_1) & \cdots & K(y_m, x_n) & K(y_m, y_1) & \cdots & K(y_m, y_m)
\end{pmatrix} = 
\begin{pmatrix}
C & K \\
K^T & S
\end{pmatrix}
$$

Пусть $m = 1$, тогда
$$
\mu = k^T C^{-1} F, \quad \sigma^2 = K(0) - k^T C^{-1} k
$$

Посмотрим на частные случаи:
$$
\lim_{y \to x_k} \mu(y)= f(x_k), \quad \lim_{y \to x_k} \sigma^2(y) = 0
$$
$$
\lim_{y \to \infty} \mu(y)= 0, \quad \lim_{y \to x_k} \sigma^2(y) = K(0)
$$

**Минусы:**
- Кубическая сложность по объему выборки
- Прогноз зависит от выбранной ковариационной функции $K(x, x')$

**Плюсы:**
- Позволяют оценить неопределенность прогноза
### Метод максимизации обоснованности

Будем выбирать ковариационную функцию методом максимизации обоснованности

Пусть $x \in \mathbb R^d$, $f(x)$ — стационарный гауссовский процесс, с непрерывной $K(0)$

**Наблюдаем** $t(x_i) = f(x_i) + \varepsilon_i, \ \varepsilon_i \sim \mathcal N \big( \varepsilon_i \mid 0, \sigma^2 \big)$

**Хотим найти** $p \big( t(x_0) \big)$

![[GaussianProcess.jpg]]

Получаем следующую вероятностную модель:
$$
p \big( \tilde T \mid T \big) = \iint p \big( \tilde T \mid \tilde F \big) p \big( \tilde F \mid F \big) \big( F \mid T \big) \ d \tilde F dF
$$
где
$$
p \big( \tilde T \mid T \big) = \mathcal N \big( t(x_0) \mid f(x_0), \sigma^2 \big)
$$
$$
p \big( \tilde F \mid F \big) = \mathcal N \big( f(x_0) \mid \mu(x_0), \sigma^2(x_0) \big)
$$
и $\mu(x_0), \sigma^2(x_0)$ определяются по формулам Андерсена
$$
p \big( F \mid T \big) = \frac{p \big( T \mid F \big) p(F)}{\int p \big( T \mid F \big) p(F) dF} = \frac{p \big( T \mid F \big) p(F)}{p(T)}
$$
и так как $p \big( T \mid F \big) = \mathcal N \big( T \mid F, \sigma^2 \big)$, $p(F) = \mathcal N \big( F \mid 0, C \big)$, то
$$
p(T) = \mathcal N \big( T \mid 0, \hat C \big), \quad c_{ij} = K(x_i - x_j) + \sigma^2 \big[ x_i - x_j \big]
$$

Далее максимизируем обоснованность $p(T)$ по некоторому параметрическому семейству $K(\cdot, \cdot)$:
$$
p \big( \tilde T \mid T \big) \to \max_{K(\cdot, \cdot), \sigma^2}
$$

Часто выбирают
$$
K(x - x') = A \exp \big( - \gamma \| x - x' \|^2 \big)
$$

Можно рассмотреть и другую модель:
![[GaussianProcess2.jpg]]
$$
K_T(x, x') = K_F(x, x') + \sigma^2 \big[ x = x' \big]
$$

### Гауссовские процессы для задачи классификации
Пусть мы наблюдаем не зашумленные значения $f(x_1), \ldots, f(x_n)$ процесса $f(x)$ а метки $t_1, \ldots, t_n \in \{ -1, 1 \}$

Тогда зададим
$$
p \big( T \mid F \big) = \prod_{i=1}^n \frac{1}{1 + \exp \big( - t_i f(x_i) \big)}
$$

- $p \big( \tilde T \mid \tilde F \big) = \sigma \big( t_0 f(x_0) \big)$
- $p \big( \tilde F \mid F \big)$ будем так же считать по формулам Андерсена
- $p \big( F \mid T \big)$ имеет вид
  $$
  p \big( F \mid T \big) = \frac{p \big( T \mid F \big) p(F)}{\int p \big( T \mid F \big) p(F) dF} \approx \mathcal N \big( F \mid \cdot, \cdot \big)
  $$
  где интеграл в знаменателе не считается, но его можно приблизить (как в RVM) c помощью $Q(f)$ такой, что
  $$
  \log Q(f) \approx \log Q(F_{MP}) + \frac{1}{2} (F - F_{MP})^T \nabla \nabla \log Q(F_{MP}) (F - F_{MP})
  $$
  $$
  F_{MP} = \arg \max_F p \big( T \mid F \big) p(F)
  $$