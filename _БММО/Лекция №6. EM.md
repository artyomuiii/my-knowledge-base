## Лекция №6. Модели с латентными переменными.

*Идея:*
Вместо $p(X|\theta)$ (**неполное правд-е**) будем работать с $p(X,Z|\theta)$ (**полное правд-е**), где $Z$ - **латентные переменные**.

*Мотивация:*
Почти всегда оптимизация полного правд-я сильно проще, чем неполного. Также латент-е перем-е часто оказываются очень интерпретируемыми.

**Граница ELBO (Evidence Lower Bound)**:  ^4a13f4
$$
log \: p(X|\theta) = KL(q(Z)||p(Z|X, \theta)) + L(q,\theta) \ge L(q,\theta)
$$
Где:
* $KL(p||q) \ge 0$ - дивергенция Кульбака-Лейблера (похожа на метрику, но не метрика).
* $L(q, \theta)$ - **вариационная нижняя граница**, а мы знаем как её использовать для оптимизации - итеративный поокординатный процесс.
    Приминительно к этой задаче этот процесс наз-ся **EM-алгоритмом**:
    $L(q, \theta) \rightarrow \underset{q}{max}$ 
    $L(q, \theta) \rightarrow \underset{\theta}{max}$ 
    Хорошо оптимизируются распределения из **экспоненциального класса**.

#### Пример. Вероятностный вывод **PCA**.
*Модель:*
$$
p(x,z|\theta) = p(x|z, \theta) p(z) = N(x|Wz + \mu, \sigma^2 I) N(z|0, I)
$$
Где:
* $x \in R^D, z \in R^d, D > d$
* $\theta=\{W, \mu, \sigma\}$

*Задача:*
$$
max_\theta p(X|\theta)
$$

*Преимущества:*
* *Решает проблему пропусков в данных:*
		$p(Obs, Hid|\theta)$, где:
		$Obs$ - наблюдаемые ($X$ + некоторые из $Z$, которые мб заданы сразу)
		$Hid$ - скрытые ($Z$ + пропущенные из $X$)
* *Автоматическое определение размера спрямляющего пр-ва:*
		Теперь $p(W|A)$. Если взять идею из **RVM**, то придём к тому, что некоторые $\alpha_j$ уходят в $inf$, обнуляя тем самым соответствующий столбец (признак) матрицы $W$. Оставшиеся столбцы образуют спрямляющее пр-во.
* *Смесь PCA:*
		$p(X,Z,T|\theta)$, где:
		$T$ - (латентные) к какому п/пр-ву относится объект.