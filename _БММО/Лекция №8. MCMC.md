## Лекция №8. Методы сэмплирования.
*"Байес для среднего класса":*
1) Факторизация ([[Лекция №7. MFA|Mean-field]])
2) Параметризация (не рассматривали)
3) Сэмплирование

Почти всегда апостериорное распред-е нужно для прогнозирования:
$$
p(t_{test}|x_{test},X_{tr}, T_{tr}) = \int p(t_{test}|x_{test},\theta) p(\theta|X_{tr}, T_{tr})d\theta
$$
Т.е. достаточно уметь сэмплировать из апостериорного распр-я, не зная его.
Сэмплирование методом **Монте-Карло** ($E_qf$ заменяется средним по значениям ф-ции $f$ в сэмплированных из распр-я $q$ точках).
* \+ несмещённая оценка
* \+ $Df$ не зав-т от размерности пр-ва
* \-  вычислительно трудно

Генерация сэмплов - трудная задача!
Умеем сэмп-ть из $U[a,b]$, конечных дискретных и распр-ий c обратимой ф.р., кот. должны знать (показательное, Коши, ...).

*Распр-е Коши: колокол с тяжёлыми хвостами => нет $E, D$ (только в смысле главного значения). Факт: усреднение не приводит к росту точности.*

***

$p(x)$ - хотим, но не умеем
$q(x)$ - умеем, но не хотим

#### 1. Rejection sampling
$Cq(x) \ge p(x)$
1. Сэмпл-ем $y$ из $q$
2. Сэмпл-ем $\theta$ из $U[0, Cq(y)]$
3. $x_{k+1} = y$, если $\theta < p(y)$
	В п.1, иначе.
* \- не все точки принимаются


#### 2. Importance sampling
$E_pf = E_qf\frac{p}{q}$
* \+ нет реплекаций
* \- есть веса, кот. уменьшают фактическое разнообразие сэмплов


#### 3. Self-normalize Importance sampling
Плотность известна с точностью до const: $p(x) = \frac{p^*(x)}{B}$
Используется трюк из Importance sampling. Результат также содержит веса.

***

*Мотивация:* Ограничение на сэмплированиее н.о.р.с.в. очень жёсткое.
Будем допускать некоторую зав-ть с.в.

**Марковская цепь (МЦ)** - факторизация: следующий зависит от предыдущего.
**Однородная МЦ** - вер-ть перехода одинаковая.
**Стационарное распр-е** - начиная с некоторого момента вер-ть очередного объекта не меняется. Не единственно и не всегда существует, зависит от начальной вер-ти.
**Эргодичная МЦ** - если у МЦ существует единственное предельное распр-е (для любой начальной вер-ти).

**Уравнение детального баланса (УДБ)**
*Экономический смысл: "Ты инвестируешь в другие страны столько же, сколько другие страны инвестируют в тебя"*

***

#### 4. Метод Метрополиса-Хастингса
Плотность известна с точностью до const: $p(x) = \frac{p^*(x)}{B}$

Есть некоторое распр-ие $r(x'|x)>0$. Следующий $x'$ сэмплируется из этого распр-я  с вер-тью $A(x,x') = min(1, \frac{\cdot}{\cdot})$, иначе оставляем предыдущий $x$. Т.о. образуется однородная эргодичная МЦ, для $p(x)$ выполнено УДБ (док-ся) => исходное $p(x)$ - стационарное распр-е, к которому сх-ся МЦ (за сколько (mixin time) - другой вопрос) и из которого после некоторого момента будут сэмплироваться желаемые $x$.

Создан во время Манхэтенского проекта.
На нём основаны все современные методы сэмплирования.
Есть корреляция, поэтому, чтобы уменьшить её, берут, например, каждый 20-ый сэмпл.


#### 5. Gibbs sampling
*Метод случайного блуждания*
Следующий сэмпл находится покоординатно: сэмплируем очередную координату из одномерного распределения (это просто сделать), зафиксировав оставшиеся координаты.
(Док-во разбиралось на семинаре)
Часто используется.

* \+ нет реплекаций
* \- не параллелится!
* \- плохо работает в случае мн-ва мод