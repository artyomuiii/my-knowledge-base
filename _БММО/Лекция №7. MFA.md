## Лекция №7. Mean-field approximation.
*Важные распределения:*
* Категориальное:
	$Cat(x|\pi), \pi$ - вероятностный симплекс
* Дирихле:
	$Dir(\pi|\alpha), \alpha_k > 0$
	Если:
	* $\alpha_k = 1$ - равномерное распр-е на симплексе
	* $\alpha_k < 1$ - поощряется разреживание
	* $\alpha_k > 1$ - аналог гауссианки на симплексе
* **Уишерта**:
	сопряжённое к многомерному нормальному с фикс. $\mu$
	$\mathbf{W}(\Lambda|W,\nu)$
	$E\Lambda = \nu W$  (чем больше $\nu$, тем меньше отклонение от $E\Lambda$)
* **Уишерт-нормальное**:
	сопряжённое к многомерному нормальному
	$p(\mu,\Lambda) = N(\mu|m,(\beta\Lambda)^{-1}) \mathbf{W}(\Lambda|W,\nu)$

В [[Лекция №6. EM|лекции №6]] рассматривался EM-алгоритм:
Работали с $p(X,Z|\theta)$ вместо $p(X|\theta)$
Искали: $\theta_{ML} = argmax[p(X|\theta)]$ 

Теперь введём распределение $p(\theta)$ на параметры $\theta$.
*Зачем?*
Например, модель разделения смеси гауссиан и в рамках задачи нужно, чтобы их было мало => используем распр-е Дирихле с $\alpha_k < 1$ => поощряем разреживание или задача ДП про мышей и эллипсы.
$p(X,Z,\theta) = p(X,Z|\theta)p(\theta)$
Теперь ищем: $\theta_{MP} = argmax[p(\theta|X)] = argmax[p(X|\theta)p(\theta)]$

Для $log[p(X|\theta)]$ у нас есть вариационная нижняя граница ELBO: $L(q,\theta)$.
Поэтому, EM'-алгоритм выглядит так:
*E-step:* без изменений
*M'-step:* ... + $log[p(\theta)]$

*Проблема:*
На E-шаге выполняется "байес для богатых" $p(Z|X,\theta)$, но он не всегда возможен. В таком случае, можно использовать **Mean-field approximation**: будем искать распр-е в факторизованном виде $q(Z) = q_1(Z_1)...q_M(Z_M)$ (каждая группа переменных принадлежит только одной ф-ции, чем меньше ф-ий, тем лучше).

Просто считая KL-див-ю между факторизованным $q(Z)$ и $p(Z|X)$ приходим к след результату:
**Основная ф-ла Mean-field approximation:**
$$
log[q_m(Z_m)] = E_{Z/m}log[p(X,Z)] + Const
$$

**Условно сопряжённое распр-е** - распр-е, априорно-сопряжённое по каждому параметру при фикс. остальных.

Между "байесом для бедных" и "байесом для богатых" - много чего есть:
MF, (EM', M'E), (MF EM', MF M'E) (по убыванию крутости).
Частные случаи MF:
* EM', если: $\delta(\theta-\theta_{MP}) p(Z|X, \theta{MP})$
* "байес для бедных", если: $\delta(\theta - \theta_{MP}) \delta(Z - Z_{MP})$

*Пример:* разделение смеси гауссиан.
*Модель:* 
$$
p(X, Z, \mu, \Lambda, \pi) = \prod_{i=1}^n p(x_i|z_i, \mu, \Lambda) p(z_i|\pi) \prod_{k=1}^K p(\mu_k, \Lambda_k) p(\pi) 
$$
$$
p(...) = \prod_{i=1}^n N(x_i|\mu_{z_i}, \Lambda_{z_i}^{-1}) \pi_{z_i} \prod_{k=1}^K N(\mu_k| m_0, (\beta_0 \Lambda_k)^{-1})\mathbf{W}(\Lambda_k|W_0, \nu_0) Dir(\pi|\alpha_0)
$$
Как раз тут примерное решение задачи про мышей.
