## Лекция №4. Метод релевантных векторов для задачи линейной регрессии. 

##### Реш-ие СЛАУ
$Ax=b$, $A$: mxn:
1. n=m
    Если А не вырождена => сущ-ет **классическое реш-ие**.
1. n\<m 
     Вообще говоря, классического реш-ия не сущ-ет.
    **Псевдорешение:** $x = (A^TA)^{-1}A^Tb$
    Здесь мат. $A^TA > 0$.
1. n>m
    Реш-ий бесконечно много. Это значит, что мат. $A^TA$ - вырождена, поэтому произведём ридж-регуляризацию.
    **Нормальное псевдорешение:** $x = lim[(A^TA + \lambda I)^{-1}A^Tb]$ при стремлении $\lambda$ к $0+$.

---

##### Задача линейной регрессии
*Модель:* $p(t, w|x) = p(t|x, w)p(w) = N(t|x^Tw, \beta^{-1})N(w|0, \alpha^{-1}I)$
Хотим найти $w_{MP}$ => получим линейную регрессию с регуляризацией или "байес для бедных".

Теперь хотим сделать "байес для богатых". Легко видеть, что распр-я сопряжённо-априорные, поэтому для полного байесовского вывода нужно пересчитать параметры норм. распр-я.
*Интерпретация:* ансамбль линейных регрессий.

НО разница между бедным и богатым байесом не существенная, т.к. модель очень простая.

---

**Улучшение модели - метод релевантных векторов (RVR)**
Пусть теперь мат. $А = diag(\alpha_1, ..., \alpha_d)$,
где $\alpha_1, ..., \alpha_d$ - гиперпараметры.
С помощью CV не получится их подобрать, т.к. очень! долго.
Выберем параметры с помощью [[Лекция №3. Наибольшая боснованность|принципа наибольшей обоснованности]].

В момент взятия аргмакса по альфам, необходимо продифференцировать очень! сложное выражение, которое такое из-за зав-ти в $w_{MP}(\alpha)$.
*Выход:* использовать **вариационную нижнюю оценку** - ф-цию с добавленным параметром кси, которая всюду не превосходит исходную и в каждой точке $x$ гарантированно достигает её на некотором параметре кси.
*Мотивация:* почти всегда вар. ниж. оценка сильно проще для оптимизации. 

Далее запускается **покомпонентный итерационный процесс**, который оптимизирует вар. ниж. оценку. Док-но, что есть сх-ть к стац. точке, которая обычно и есть оптимум. EM-алгоритм - его частный случай.

После вывода аналитических формул, мы получаем нелинейные ур-я относ-но альф. Используем **метод простой итерации (МПИ)** и получим формулы для пересчёта.

Обычно получаются неприятные для оптимизации ф-ции, поэтому **покоординатно логарифмируют** и итерируются уже в новом пр-ве. Также можно менять параметры, взятые с предыдущего шага (new, old), но есть сложившаяся схема, показывающая самую быструю сх-ть на практике.

*Результат:* на практике **RVM обнуляет неинформативные и шумовые признаки**, устремляя их альфы в бесконечность. Почему так? Есть разные эвристики, но нет строгого док-ва.