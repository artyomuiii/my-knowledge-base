# ReLU
**ReLU** - ф-ция активации вида:
$$
g(x)=\max (0, x)
$$

![[Pasted image 20220318173627.png]]

**Преимущества:**
* Градиент не затухает на положительной полуоси.
* Наиболее вычислительно эффективен (операция max(0, ...) очень простая).
* Разреженность данных после применения (хотя бывают случаи, когда это не плюс).

**Улучшения:**
* [[LeakyReLU]]
* [[ELU]]
* [[ISRLU]]
* [[PReLU]]
* [[RReLU]]
* [[SReLU]]