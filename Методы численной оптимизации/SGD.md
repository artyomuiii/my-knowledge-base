## SGD
**SGD** - метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента, вычисленного по случайной подвыборке объектов (по **батчу**).

$$
w^{i+1}=w^{i}-\epsilon_{i} \nabla_{w} {Q}(w)=w^{i}-\epsilon_{i} \Delta^{i}
$$
$$
\Delta^{i}=\frac{1}{\left|I_{i}\right|} \sum_{j \in I_{i}} \nabla_{w} {Q}^{j}(w), \quad I_{i} \sim \mathcal{U}[\mathfrak{B}]
$$
Где $\mathfrak{B}$ - множество всех возможных батчей размера $B$ без повторений элементов внутри батча.

**Преимущества:**
* Быстрое вычисление.

**Недостатки:**
* Сильно осцилирует => медленная сходимость.
* Не сходится к локальному оптимуму, необходимо параллельно уменьшать шаг обучения.
* Имеется регион неопределённости. (TODO)

**Улучшения:**
* [[SGD with Momentum]]
* [[RMSProp]]